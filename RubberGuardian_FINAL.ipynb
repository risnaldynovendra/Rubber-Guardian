{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UADXt34edMfU",
        "outputId": "99cd8af4-6464-4793-cc71-696b6e0f130c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "PL1C2akz3BsS",
        "outputId": "6ea23a88-f158-47a7-f2ac-7515a94a2387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ab81bd0-b8f4-43ee-b00b-1c75988f10b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ab81bd0-b8f4-43ee-b00b-1c75988f10b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "AmjMoXQY3CYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ftAD7mVn3D5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "jvztZ10b3Fhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14f76b0-00ec-4fc0-f738-9b974d11e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hBA8Fj853HdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18449f64-bf1c-4a17-d69c-cb1e8e451f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kEdfSPF3JGJ",
        "outputId": "5c247c76-88ab-481e-e8ae-51d6849c4e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d warcoder/tyre-quality-classification"
      ],
      "metadata": {
        "id": "NCD4Fd5o3L78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "89olSSU93dS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip =  zipfile.ZipFile('tyre-quality-classification.zip', 'r')\n",
        "\n",
        "dataset_zip.extractall()\n",
        "dataset_zip.close()"
      ],
      "metadata": {
        "id": "8XyG5t313f3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khgbfr1AWUWa"
      },
      "outputs": [],
      "source": [
        "Id=[]\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/Digital images of defective and good condition tyres'):\n",
        "    for filename in filenames:\n",
        "        Id.append(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.DataFrame()\n",
        "train=train.assign(filename=Id)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "em6AizJ32ICS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['label']=train['filename']\n",
        "train['label']=train['label'].str.replace('/content/Digital images of defective and good condition tyres/','')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "73T-I4Sa2I5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['label'] = train['label'].str.split('/').str[0]\n",
        "train.head()"
      ],
      "metadata": {
        "id": "VgfWRlLm2K9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=(train.label.value_counts()).to_frame()\n",
        "count.reset_index(inplace=True)\n",
        "count.columns=['label','count']\n",
        "count"
      ],
      "metadata": {
        "id": "ddjjtGM_2M9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test= train_test_split( train, test_size=0.2, random_state=42,shuffle=True, stratify=train['label'])"
      ],
      "metadata": {
        "id": "KAC6sCl12O32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=(train.label.value_counts()).to_frame()\n",
        "count.reset_index(inplace=True)\n",
        "count.columns=['label','count']\n",
        "count"
      ],
      "metadata": {
        "id": "WeocDiRQ2Sgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=train.sample(n=len(train))\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "j4dpR6q-2UVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=test.sample(n=len(test))\n",
        "test.reset_index(inplace=True,drop=True)\n",
        "test.head()"
      ],
      "metadata": {
        "id": "mnhe1Oeq2Wcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "def _process_df_chunk(chunk, file_col, label_col, dest_folder):\n",
        "    def _process_image(file_path, label):\n",
        "        try:\n",
        "            # Load the image\n",
        "            img = cv2.imread(file_path)\n",
        "            # Check if the image size is empty\n",
        "            if img is None or img.size == 0:\n",
        "                print(f\"Error: Could not read file {file_path}\")\n",
        "                return\n",
        "            # Resize to 512x512\n",
        "            img = cv2.resize(img, (512, 512))\n",
        "            # Save the image to the folder with label name\n",
        "            folder_path = os.path.join(dest_folder, str(label))\n",
        "\n",
        "            if not os.path.exists(folder_path):\n",
        "                os.makedirs(folder_path)\n",
        "            file_name = os.path.basename(file_path)\n",
        "            # Append a unique identifier to the file name to avoid overwriting\n",
        "            name, extension = os.path.splitext(file_name)\n",
        "            save_path = os.path.join(folder_path, f\"{name}_{uuid.uuid4().hex}{extension}\")\n",
        "            cv2.imwrite(save_path, img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    for _, row in chunk.iterrows():\n",
        "        _process_image(row[file_col], row[label_col])\n",
        "\n",
        "def resize_save_image(df, file_col, label_col, dest_folder):\n",
        "    try:\n",
        "        # Determine the number of processes to use\n",
        "        num_processes = multiprocessing.cpu_count()\n",
        "\n",
        "        df_chunks = [df[i:i + num_processes] for i in range(0, len(df), num_processes)]\n",
        "\n",
        "        # Create a partial function with fixed arguments\n",
        "        func = partial(_process_df_chunk, file_col=file_col, label_col=label_col, dest_folder=dest_folder)\n",
        "\n",
        "        # Run the function in parallel on the DataFrame chunks\n",
        "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "            pool.map(func, df_chunks)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "5xHQALjA2cXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newpath = r'./tyre-quality-classification/train'\n",
        "if not os.path.exists(newpath):\n",
        "    os.makedirs(newpath)"
      ],
      "metadata": {
        "id": "65uUI47I2qjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_save_image(train, file_col='filename', label_col='label' ,dest_folder='./tyre-quality-classification/train')"
      ],
      "metadata": {
        "id": "FeocpOFW2swo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newpath = r'./tyre-quality-classification/test'\n",
        "if not os.path.exists(newpath):\n",
        "    os.makedirs(newpath)"
      ],
      "metadata": {
        "id": "NwYAMuJz2tlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_save_image(test, file_col='filename', label_col='label' ,dest_folder='./tyre-quality-classification/test')"
      ],
      "metadata": {
        "id": "ocwSEs7k2vQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "F0Fu5La2XyqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "2orjKryWYNRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "7-Z_8Ao-X7Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "1qMsZhZlYR0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "import PIL\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score, classification_report\n",
        "from numpy.random import seed"
      ],
      "metadata": {
        "id": "OZsJUC76VbsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "Ifq7SX9LVhdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.GaussianBlur(5, sigma=0.60),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "Iu13KhqhLQvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/tyre-quality-classification/train'\n",
        "test_dir = '/content/tyre-quality-classification/test'\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=32, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=32, pin_memory=True)"
      ],
      "metadata": {
        "id": "tdFcW4sGYC_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "def show_transformed_img(dataset):\n",
        "  loader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\n",
        "  batch = next(iter(loader))\n",
        "  images, labels = batch\n",
        "\n",
        "  grid = torchvision.utils.make_grid(images, nrow = 3)\n",
        "  plt.figure(figsize=(11,11))\n",
        "  plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "  print('labels', labels)"
      ],
      "metadata": {
        "id": "KKLqVLqEbmH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_transformed_img(test_data)"
      ],
      "metadata": {
        "id": "eVgSR7JgY8rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_device():\n",
        "  if torch.cuda.is_available():\n",
        "    dev = 'cuda:0'\n",
        "  else:\n",
        "    dev = 'cpu'\n",
        "\n",
        "  return torch.device(dev)"
      ],
      "metadata": {
        "id": "UacBGRzvaORa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs ):\n",
        "  device = set_device()\n",
        "  best_acc = 0\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Epoch number %d\" % (epoch + 1))\n",
        "    model.train\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      running_correct += (labels == predicted).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss/len(train_loader)\n",
        "    epoch_acc = 100.0 * running_correct / total\n",
        "\n",
        "    print(\"     - Training dataset. Got %d out of %d images correctly (%.3f%%). Epoch loss: %.3f\" % (running_correct, total, epoch_acc, epoch_loss))\n",
        "\n",
        "    test_dataset_acc = evaluate_model_on_test_set(model, test_loader)\n",
        "\n",
        "    if(test_dataset_acc > best_acc):\n",
        "      best_acc = test_dataset_acc\n",
        "      save_checkpoint(model, epoch, optimizer, best_acc)\n",
        "\n",
        "  print(\"Finished\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "uz_IVEJKa-uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_on_test_set(model, test_loader):\n",
        "  model.eval()\n",
        "  predicted_correctly_on_epoch = 0\n",
        "  total = 0\n",
        "  device = set_device()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      predicted_correctly_on_epoch += (predicted ==labels).sum().item()\n",
        "\n",
        "  epoch_acc = 100.0 * predicted_correctly_on_epoch / total\n",
        "  print(\"     - Testing dataset. Got %d out of %d images correctly (%.3f%%)\" % (predicted_correctly_on_epoch, total, epoch_acc))\n",
        "\n",
        "  return epoch_acc"
      ],
      "metadata": {
        "id": "df_T6RrmfQ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, epoch, optimizer, best_acc):\n",
        "  state = {\n",
        "      'model': model.state_dict(),\n",
        "      'epoch': epoch + 1,\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'best_accuracy': best_acc,\n",
        "  }\n",
        "  torch.save(state, 'rubber_guardian_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "NLca-nSpUXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "resnet50_model = models.resnet50(pretrained = True)\n",
        "num_classes = 2\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting=True):\n",
        "    if feature_extracting:\n",
        "        for name, param in model.named_parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "resnet50_model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n",
        "\n",
        "device = set_device()\n",
        "resnet_50_model = resnet50_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9, weight_decay=0.003)\n",
        "\n"
      ],
      "metadata": {
        "id": "w0yJi0EchPpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nn(resnet50_model, train_loader, test_loader, loss_fn, optimizer, 150)"
      ],
      "metadata": {
        "id": "E8pkUyVJi6mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('/content/rubber_guardian_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "CuqHDL12AtT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(checkpoint['epoch'])\n",
        "print(checkpoint['best_accuracy'])"
      ],
      "metadata": {
        "id": "HT1Nhq-8ByoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_model = models.resnet50()\n",
        "num_classes = 2\n",
        "resnet50_model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n",
        "resnet50_model.load_state_dict(checkpoint['model'])"
      ],
      "metadata": {
        "id": "zQNuMhYICCdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(resnet50_model, 'best_rgmodel2.pth')"
      ],
      "metadata": {
        "id": "bpdvJzniWkb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/best_rgmodel2.pth')"
      ],
      "metadata": {
        "id": "BOcym-pkIobj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import PIL.Image as Image"
      ],
      "metadata": {
        "id": "7t_xkRQEMULA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"defective\",\n",
        "    'good'\n",
        "]"
      ],
      "metadata": {
        "id": "khAHhu98MjgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/best_rgmodel.pth')"
      ],
      "metadata": {
        "id": "SN5YKuH5O600"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, image_transforms, image_path, classes):\n",
        "  model = model.eval()\n",
        "  image = Image.open(image_path)\n",
        "  image = image_transforms(image).float()\n",
        "  image = image.unsqueeze(0)\n",
        "\n",
        "  output = model(image)\n",
        "  _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "  print(classes[predicted.item()])"
      ],
      "metadata": {
        "id": "Nu9E9sq7NdIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/Defective (7).jpg\", classes)"
      ],
      "metadata": {
        "id": "IzK7iyt9Oor9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/Defective (63).jpg\", classes)"
      ],
      "metadata": {
        "id": "M-YZdH_SPHHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/Defective (45).jpg\", classes)"
      ],
      "metadata": {
        "id": "hJpgX9PBPad5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/Defective (2).jpg\", classes)"
      ],
      "metadata": {
        "id": "jxl0cbXTPiJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/Defective (11).jpg\", classes)"
      ],
      "metadata": {
        "id": "p0r6cEFyPlU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/good (1).jpg\", classes)"
      ],
      "metadata": {
        "id": "hxWjGmCYPni_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/good (542).jpg\", classes)"
      ],
      "metadata": {
        "id": "i1BOigkSQH8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/good (63).jpg\", classes)"
      ],
      "metadata": {
        "id": "2N70ZU60QKle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/good (682).jpg\", classes)"
      ],
      "metadata": {
        "id": "7ke2jAbcQMoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/good (9).jpg\", classes)"
      ],
      "metadata": {
        "id": "KrFb8vrPQOqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/google_tyre_defect(1).jpg\", classes)"
      ],
      "metadata": {
        "id": "sdMSZy0iRrti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify(model, test_transform, \"/content/google_tyre_good(1).jpg\", classes)"
      ],
      "metadata": {
        "id": "STi40VTjTEUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}